---
title: "Deep learning in mlr"
author: "Jannek Thomas"
date: "13. 06 2018"
output:
  ioslides_presentation:
    css: txt_slides.css
    includes:
      in_header: header.html
    theme: united
    toc: yes
    widescreen: no
  beamer_presentation: default
  slidy_presentation: default
bibliography: lit.bibtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(reshape2)
library(ggplot2)
library(mlr)
library(mlrMBO)
library(mxnet)
```

## Software

* Packages required for this demo are mainly __mxnet, mlr__ and __mlrMBO__
* The learner for classification tasks with mxnet in mlr is still in development mode, thus include it via
```{R, echo = TRUE}
source("https://raw.githubusercontent.com/mlr-org/mlr-extralearner/master/R/RLearner_classif_mxff.R")
```
* Follow the instructions on [the mxnet page](https://mxnet.incubator.apache.org/install/index.html) for the installation


## Deep learning with mlr?

Yes: 
 
* Methods for tuning, resampling, benchmarking and visualization are implemented within an unified framework
* Mxnet offers a performant interface which is directly executed in c++
* Parallelized, distributed GPU computation is also implemented

No:  

* R implementations tend to be one step behind their python counterparts
* No stage loading methods such as the data generator available yet
* Specification of parameter settings is currently a little cumbersome


## Data: fashion MNIST

* Dataset offered by [Zalando](https://github.com/zalandoresearch/fashion-mnist)
* Consists of 70K images of 10 different fashion items such as pullovers, shirts, shoes...
* Images are grayscale and in format 28x28 as with [MNIST](http://yann.lecun.com/exdb/mnist/)
* Split in 60K (6K/ class) and 10K balanced test data

## Data: fashion MNIST

```{R, echo = FALSE, cache = TRUE, eval = TRUE, fig.cap = "Example data from fashion MNIST"}
# get data from github and store in file in repo
# system(command = "cd .. &&
#   rm -rf data &&
#   mkdir data && 
#   cd data && 
#   wget https://github.com/zalandoresearch/fashion-mnist/raw/master/data/fashion/t10k-images-idx3-ubyte.gz && 
#   wget https://github.com/zalandoresearch/fashion-mnist/raw/master/data/fashion/t10k-labels-idx1-ubyte.gz && 
#   wget https://github.com/zalandoresearch/fashion-mnist/raw/master/data/fashion/train-images-idx3-ubyte.gz && 
#   wget https://github.com/zalandoresearch/fashion-mnist/raw/master/data/fashion/train-labels-idx1-ubyte.gz &&
#   gunzip *.gz")
# function to load data from downloaded files in R
# mainly stolen from this guy: https://markusthill.github.io/ml/programming/zalandos-fashion-mnist-dataset/

load_image_file <- function(filename) {
  ret = list()
  f = file(filename,'rb')
  readBin(f,'integer',n = 1,size = 4,endian = 'big')
  ret$n = readBin(f,'integer',n = 1,size = 4,endian = 'big')
  nrow = readBin(f,'integer',n = 1,size = 4,endian = 'big')
  ncol = readBin(f,'integer',n = 1,size = 4,endian = 'big')
  x = readBin(f,'integer',n = ret$n*nrow*ncol,size = 1,signed = F)
  ret$x = matrix(x, ncol = nrow*ncol, byrow = T)
  close(f)
  ret
}

load_label_file <- function(filename) {
  f = file(filename,'rb')
  readBin(f,'integer',n = 1 ,size = 4,endian = 'big')
  n = readBin(f,'integer',n = 1,size = 4,endian = 'big')
  y = readBin(f,'integer',n = n,size = 1,signed = F)
  close(f)
  y
}

train = load_image_file('../data/train-images-idx3-ubyte')
test = load_image_file('../data/t10k-images-idx3-ubyte')
train$y = load_label_file('../data/train-labels-idx1-ubyte')
test$y = load_label_file('../data/t10k-labels-idx1-ubyte')

data = as.data.frame(rbind(cbind(train$x, train$y), cbind(test$x, test$y)))
colnames(data) = c(colnames(data)[-785], "y")


show_digit <- function(arr784, col = gray(12:1/12), ...) {
  image(matrix(arr784, nrow = 28)[,28:1], col = col, ...)
}

showExamples <- function(nExamplesClass = 10, randomize = F) {
  classes <- sort(unique(train$y)) # a vector of all classes
  nClasses <- length(classes) # Number of classes
  iExamples <- as.matrix(sapply(classes,
                                function(i) {
                                  iLab <- which(train$y == i)
                                  sample(x = iLab, size = nExamplesClass, replace = F)
                                }))
  if (randomize) {
    iExamples <- matrix(sample(x = iExamples, size = length(iExamples), replace = F),
                  nrow = dim(iExamples)[1])
  }

  data <- array(train$x[iExamples, ], dim = c(nExamplesClass * nClasses, 28, 28))
  plotData <- melt(data, varnames = c("image", "x", "y"), value.name = "pixel")
  nrows <- if (randomize) round(sqrt(nExamplesClass * nClasses)) else 10
  p <- ggplot(plotData, aes(x = x, y = y, fill = pixel)) +
    geom_tile() +
    scale_y_reverse() +
    facet_wrap(~ image, nrow = nrows) +
    theme_bw() +
    theme(
      panel.spacing = unit(0, "lines"),
      axis.text = element_blank(),
      axis.ticks = element_blank(),
      strip.background = element_blank(),
      strip.text.x = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      legend.position   = "none",
      axis.title.x = element_blank(),
      axis.text.x = element_blank(),
      axis.ticks.x = element_blank(),
      axis.title.y = element_blank(),
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank()
    ) + scale_fill_gradient(low = "black", high = "white")

  plot(p)
}

# get data
showExamples(nExamplesClass = 10, randomize = TRUE)
```



## Simple CNN in mlr

```{R, echo = TRUE}
# reshape the data
# does not work as mlr does NOT accept matrices, only data frames with given shape
task_data = as.matrix(data)[, -785]
dim(task_data) = c(nrow(task_data), 28, 28, 1)

#task_data[, , , 1] = data$y
#task_data = as.data.frame(task_data)

# define task
task = makeClassifTask(id = "fmnist", data = data, target = "y")

# set architecture as symbol
sym = mx.symbol.Variable("data")
sym = mx.symbol.Convolution(sym, kernel = c(3, 3), num_filter = 5)
sym = mx.symbol.Activation(sym, act_type = "tanh")
sym = mx.symbol.Pooling(sym, kernel = c(2, 2), stride = c(2, 2))
sym = mx.symbol.Flatten(sym)
sym = mx.symbol.FullyConnected(sym, num_hidden = 100)
sym = mx.symbol.Activation(sym, act_type = "tanh")
sym = mx.symbol.SoftmaxOutput(sym)

mx.model.FeedForward.create(symbol = sym, X = task_data[, , , -1], )

# define the learner
lrn = makeLearner(cl = "classif.mxff", 
  # architecture
  symbol = sym,
  verbose = TRUE, 
  conv.data.shape = c(28, 28, 1),
  array.layout = "rowmajor",
  # training specs
  optimizer = "sgd", 
  eval.metric = mx.metric.accuracy, 
  validation.ratio = 0.2, 
  # use ES
  epoch.end.callback = mx.callback.early.stop(bad.steps = 5, maximize = TRUE),
  # choose your weapon
  ctx = mx.cpu, 
  # epochs
  num.round = 2, 
  kvstore = "local"
)

```


```{R, echo = TRUE, eval = FALSE}
# Alternative: imperative model
# define the learner
ctx = mxnet::mx.cpu

lrn = makeLearner(cl = "classif.mxff", 
  # architecture
  #layers = 2, 
  conv.layer1 = TRUE, 
  conv.kernel1 = c(3, 3), 
  act1 = "tanh", 
  pool.kernel1 = c(2, 2), 
  pool.stride1 = c(2, 2), 
  # flatten is done automatically
  num.layer2 = 100, 
  act2 = "tanh",
  verbose = TRUE, 
  conv.data.shape = c(28, 28),
  #array.layout = "colmajor",
  # training specs
  optimizer = "sgd", 
  eval.metric = mx.metric.accuracy, 
  validation.ratio = 0.2, 
  # use ES
  epoch.end.callback = mx.callback.early.stop(bad.steps = 5, maximize = TRUE),
  # choose your weapon
  ctx = ctx, 
  # epochs
  num.round = 2, 
  kvstore = "local"
)
```




```{R, echo = FALSE, eval = FALSE, eval = FALSE}
mod = train(learner = lrn, task = task, subset = c(1:60000))

preds = predict(mod, task = task, subset = 60001:70000)
# performance(pred = preds, measures = mmce)

```

## Simple CNN in mlr

```{R, echo = FALSE, out.width="100%", fig.align='center', fig.cap="Illustration of the simple mxnet's architecture", warning=FALSE, message=FALSE}
graph.viz(symbol = sym, direction = "LR", graph.height.px = 150, type = "graph")
```


## Tuning the simple CNN


* There are several parameters in the architecture that we want to tune
* Set up the parameter space
```{R, echo = TRUE, eval = FALSE}

par.set = makeParamSet(
  makeNumericParam(id = "learning.rate", lower = 0.05, upper = 0.3), 
  makeNumericParam(id = "momentum", lower = 0.7, upper = 0.99), 
  makeIntegerParam(id = "num.layer2", lower = 10, 500), 
  makeDiscreteParam(id = "conv.kernel1", c(c(3, 3), c(5, 5), c(2, 2))), 
  makeDiscreteParam(id = "act1", c("tanh", "relu", "sigmoid"))
)
```
* Tune the model with MBO
* possible budget constraints:
    * __iters:__ number of sequential optimization steps
    * __time.budget:__ time after which the search stops [sec]
    * __target.fun.value:__ search stops when this performance-value is reached

```{R, echo = TRUE, eval = FALSE}

ctrl = makeMBOControl()
# set time budget in seconds
ctrl = setMBOControlTermination(ctrl, time.budget = 60)
tune.ctrl = makeTuneControlMBO(mbo.control = ctrl)
result = tuneParams(learner = lrn, task = task, resampling = cv3, 
  par.set = par.set, control = tune.ctrl, show.info = TRUE)
```

## Tuning the simple CNN

* Access the results and train model with the tuned parameter setting
```{R, echo = TRUE, eval = FALSE}
# set hyper parameters
lrn.tuned = setHyperPars(makeLearner(cl = "classif.mxff"), par.vals = result$x)
mod.tuned = mlr::train(learner = lrn, task = task, subset = c(1:60000))
# predict and test performance
preds = predict(mod.tuned, task = task, subset = 60001:70000)
performance(pred = preds, measures = mmce)
```

## Fine-tune ResNet in mlr

* Use famous ResNet architecture 


```{R, echo = FALSE}
source("https://raw.githubusercontent.com/apache/incubator-mxnet/master/example/image-classification/symbol_resnet-28-small.R")
mx.thing = get_symbol()
graph.viz(symbol = mx.thing, type = "graph")

```



# CUT



<!-- ## Why dl in mlr? -->

<!-- * __Tuning, resampling, benchmarking__ -->
<!-- * deep neural nets consist of a magnitude of different hyperparameters -->
<!-- * finding the optimal parameter setting is critical for a strong performance -->
<!-- * training and testing of neural nets is very time consuming -->
<!-- * sophisticated methods required to tune neural nets efficiently given budget constraints -->
<!-- * `mlr` with `mlrMBO` offers a reliable and efficient environment for tuning of `mxnets` -->

<!-- ## What are the pitfalls of using mlr for dl? -->

<!-- * no stage loading -->
<!-- * ... -->


<!-- ## Data bye bye  -->


<!-- * __only fashion mnist for everything__ -->
<!-- * __Sonar:__ -->
<!--     * 100 train and 50 balanced test data -->
<!--     * 60 numeric features  -->
<!--     * Binary classification task -->
<!-- * __Iris:__  -->
<!--     * 100 train and 50 balanced test data -->
<!--     * 4 numeric features  -->
<!--     * 3-class classification task -->
<!-- * __MNIST:__ -->
<!--     * ... -->

<!-- ```{R, echo = FALSE, warning=FALSE, cache=TRUE, message=FALSE} -->
<!-- library(dplyr) -->
<!-- data(Sonar, package = "mlbench") -->
<!-- Sonar[,61] = as.numeric(Sonar[,61]) - 1 -->
<!-- train.ind = c(1:50, 100:150) -->
<!-- train.x = data.matrix(Sonar[train.ind, 1:60]) -->
<!-- train.y = Sonar[train.ind, 61] -->
<!-- test.x = data.matrix(Sonar[-train.ind, 1:60]) -->
<!-- test.y = Sonar[-train.ind, 61] -->
<!-- ``` -->

<!-- ## Architecture building and training with mxnet -->

<!-- __no mxnet intro, only tuning/ running in mlr__ -->

<!-- Build simple multilayer perceptron (vanilla feed forward net) for classification of the sonar data: -->

<!-- ```{r, echo = TRUE, out.width="100%", fig.align='center', fig.cap="", warning=FALSE, message=FALSE} -->
<!-- # load libraries -->
<!-- library(mlr) -->
<!-- library(mxnet) -->
<!-- library(mlrMBO) -->

<!-- # set imperative model architecture and also train it -->
<!-- mod.imp = mx.mlp(data = train.x, label = train.y,  -->
<!--     hidden_node = c(5, 5), out_node = 2, out_activation = "softmax", -->
<!--     num.round = 3, array.batch.size = 15, learning.rate = 0.07, optimizer = "sgd", -->
<!--     eval.metric = mx.metric.accuracy) -->

<!-- # predict class of the first three test obsverations after 3 epochs of training -->
<!-- predict(mod.imp, test.x)[2, 1:3] -->
<!-- ``` -->


<!-- ## examples -->

<!-- 1. self set architecture on fmnist -->

<!-- 2. get resnet symbol, fine tune some of the params -->

<!-- 3. tune those two models -->

<!-- 3. outlook: continuation of training -->

<!-- ## resnet in mlr -->

<!-- * `source("https://raw.githubusercontent.com/apache/incubator-mxnet/master/example/image-classification/symbol_resnet-28-small.R")` -->
<!-- * `mx.thing = get_symbol()` -->

<!-- ## Architecture building and training with mxnet -->

<!-- ```{r, echo = FALSE, out.width="100%", fig.align='center', fig.cap="Illustration of the simple mxnet's architecture", warning=FALSE, message=FALSE} -->
<!-- graph.viz(symbol = mod.imp$symbol, direction = "LR", graph.height.px = 150) -->
<!-- ``` -->

<!-- ## Mxnet in mlr -->

<!-- * self set cnn  -->
<!-- * mlr Syntax: -->
<!--     * __task:__ combination of data, label and error measure for the given task -->
<!--     * __learner:__ object that combines an algorithm with a task -->
<!-- * setup of our previous model in mlr: -->
<!-- ```{r, echo = TRUE, out.width="100%", fig.align='center', fig.cap="Illustration of the simple mxnet's architecture", warning=FALSE, message=FALSE} -->
<!-- lrn = makeLearner("classif.mxff", layers = 2, num.layer1 = 5, num.layer2 = 5,  -->
<!--     act1 = "tanh", act2 = "tanh", act.out = "softmax")  -->
<!-- ``` -->

<!-- * hyperparameter setting of the learner: -->
<!-- ```{r, echo = FALSE, out.width="100%", fig.align='center', fig.cap="Illustration of the simple mxnet's architecture", warning=FALSE, message=FALSE} -->
<!-- hypers = getHyperPars(lrn) -->
<!-- print(as.data.frame(hypers[-c(2,3)])) -->
<!-- #knitr::kable(as.data.frame(hypers[-c(2,3)])) -->
<!-- #kable(dt) %>% -->
<!-- #    kable_styling(bootstrap_options = c("striped", "hover")) -->
<!-- ``` -->


<!-- ## Tuning mxnets in mlr with MBO -->

<!-- * setup the learner and the task -->
<!-- ```{r, echo = TRUE, out.width="100%", fig.align='center', fig.cap="Illustration of the simple mxnet's architecture", warning=FALSE, message=FALSE, cache = FALSE} -->
<!-- task = mlr::iris.task -->
<!-- lrn = makeLearner("classif.mxff", eval.metric = mx.metric.accuracy, num.round = 10) -->
<!-- ``` -->

<!-- * define the parameter space -->

<!-- ```{r, echo = TRUE, out.width="100%", fig.align='center', fig.cap="Illustration of the simple mxnet's architecture", warning=FALSE, message=FALSE, cache = FALSE} -->
<!-- par.set = makeParamSet( -->
<!--   makeIntegerParam(id = "layers", lower = 1L, upper = 2L), -->
<!--   makeIntegerParam(id = "num.layer1", lower = 5L, upper = 10L), -->
<!--   makeIntegerParam(id = "num.layer2", lower = 5L, upper = 10L, -->
<!--   # only tune neurons in layer2 if layer1 exists -->
<!--     requires = quote(layers > 1)), -->
<!--   makeDiscreteParam(id = "act1", c("tanh", "relu", "sigmoid")), -->
<!--   makeDiscreteParam(id = "act2", c("tanh", "relu", "sigmoid"), -->
<!--   # only tune activation function in layer2 if layer1 exists -->
<!--     requires = quote(layers > 1)), -->
<!--   makeNumericParam(id = "learning.rate", lower = 0.05, upper = 0.3) -->
<!-- ) -->
<!-- ``` -->

<!-- ## Tuning mxnets in mlr with MBO -->




<!-- ```{r, echo = TRUE, out.width="100%", fig.align='center', fig.cap="Illustration of the simple mxnet's architecture", warning=FALSE, message=FALSE, cache = FALSE} -->
<!-- ctrl = makeMBOControl() -->
<!-- # ctrl = setMBOControlTermination(ctrl, iters = 5) -->
<!-- ctrl = setMBOControlTermination(ctrl, time.budget = 60) -->
<!-- tune.ctrl = makeTuneControlMBO(mbo.control = ctrl) -->
<!-- ``` -->


<!-- * tune with 3-fold CV -->

<!-- ```{r, echo = TRUE, out.width="100%", fig.align='center', fig.cap="Illustration of the simple mxnet's architecture", warning=FALSE, message=FALSE, cache = FALSE} -->
<!-- res.mbo = tuneParams(learner = lrn, task = task, resampling = cv3, par.set = par.set, -->
<!--   control = tune.ctrl, show.info = FALSE) -->
<!-- ``` -->

<!-- ## Tuning mxnets in mlr with MBO -->

<!-- * inspect resulting parameter set and performance -->

<!-- ```{r, echo = TRUE, out.width="100%", fig.align='center', fig.cap="Illustration of the simple mxnet's architecture", warning=FALSE, message=FALSE, cache = FALSE} -->
<!-- print(as.data.frame(c(res.mbo$x, res.mbo$y))) -->
<!-- ``` -->

<!-- ## Alternative tuning methods in mlr -->

<!-- * macht keinen sinn ohne benchmark, cv etc -->
<!-- * randomly tune with 10 evaluations -->

<!-- ```{r, echo = TRUE, out.width="100%", fig.align='center', fig.cap="Illustration of the simple mxnet's architecture", warning=FALSE, message=FALSE, cache = FALSE} -->
<!-- rand.ctrl = makeTuneControlRandom(budget = 20L) -->
<!-- res.random = tuneParams(learner = lrn, task = task, resampling = cv3, par.set = par.set, -->
<!--   control = rand.ctrl, show.info = FALSE) -->
<!-- print(as.data.frame(c(res.random$x, res.random$y))) -->
<!-- ``` -->

<!-- * tune with iterated racing for max 20 function evaluations -->

<!-- ```{r, echo = TRUE, eval = FALSE, out.width="100%", fig.align='center', fig.cap="Illustration of the simple mxnet's architecture", warning=FALSE, message=FALSE, cache = FALSE} -->
<!-- irace.ctrl = makeTuneControlGenSA(n.instances = 10L, budget = 20L) -->
<!-- res.irace = tuneParams(learner = lrn, task = task, resampling = cv3, par.set = par.set, -->
<!--   control = irace.ctrl, show.info = FALSE) -->
<!-- print(as.data.frame(c(res.irace$x, res.irace$y))) -->
<!-- ``` -->

<!-- ## Deploy tuning results -->

<!-- ```{r, echo = TRUE, eval = TRUE, out.width="100%", fig.align='center', fig.cap="Illustration of the simple mxnet's architecture", warning=FALSE, message=FALSE, cache = FALSE} -->
<!-- # assign tuned parameter set -->
<!-- lrn.tuned = setHyperPars(makeLearner(cl = "classif.mxff"), par.vals = res.mbo$x) -->
<!-- # split -->
<!-- train.ids = sample(1:task$task.desc$size, 100) -->
<!-- test.ids = which(!(1:task$task.desc$size %in% train.ids)) -->
<!-- # train final model  -->
<!-- mod.tuned = mlr::train(learner = lrn.tuned, task = task, subset = train.ids) -->
<!-- # predict and evaluate performance -->
<!-- preds = predict(mod.tuned, task = iris.task, subset = test.ids) -->
<!-- performance(pred = preds, measures = mmce) -->
<!-- ``` -->

<!-- ## Discussion -->

<!-- * conv net example with MNIST? Timing? Prio2 imho -->
<!-- * deployment -->
<!-- * easy way to get model symbol from mlr object for graph.viz? getlearnermodel done -->
<!-- * imperative vs. symbolic programming with mxnet: include? Prio1 imho -->
<!-- * installation? Source code for learner only from git -->






